{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import two_player_game1 as tpg1\n",
    "import two_player_game2 as tpg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = 0\n",
    "low = 1\n",
    "actions = [high, low] #player can choose to invest high effort or low effort each round\n",
    "\n",
    "ph = 0.55 #Probability of winning if player invests high effort\n",
    "pl = 0.45 #Probability of winning if player invests low effort\n",
    "ch = 50 #Cost of investing high effort\n",
    "cl = 10 #Cost of investing low effort\n",
    "R = 1000 #Reward for winning (i.e. reaching state d=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.)  Iterative Evaluation of all Deterministic Policies\n",
    "\n",
    "**NOTE: Throughout this analysis, the actions will be encoded as follows: High Effort = 0 and Low Effort = 1**\n",
    "\n",
    "There are 5 non-terminal states in the environment, corresponding to $d \\in [-2,-1,0,1,2]$ and 2 terminal states, where $d = \\pm 3$. Considering these 5 non-terminal states, since there are two possible actions (high and low effort) that can be taken in each state, there are $2^5 = 32$ possible deterministic policies. (For example, one such possible is exerting high effort in every state.)\n",
    "\n",
    "Using iterative policy evaluation, we can see that the optimal policy, which is the one resulting in the highest value for the initial state of $d=0$, is $[low, low, low, high, high]$. Under this deterministic policy, if the player is tied with or has won fewer games than his opponent ($-3<d\\le0$), he exerts low effort, and if he has won more games ($0<d<3$), he exerts high effort. Assuming that the reward for winning the game is 1,000 and that the player follows this optimal policy, then the value of the initial state of this game is about 281.65. The bar plot belows shows that the most lucrative policies all involve exerting low effort when tied or losing, at least for the specified parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy is (1, 1, 1, 0, 0)\n",
      "Initial state's value under this policy is 281.6528902232666\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def iter_policy_eval(policy):\n",
    "    theta = 1e-6\n",
    "    delta = 0\n",
    "    gamma = 1\n",
    "    state_vals = [0,0,0,0,0,0,0] \n",
    "    termination = False\n",
    "    while(not termination):\n",
    "        delta = 0.0\n",
    "        #Store copy of current state values\n",
    "        v_old = np.copy(state_vals) \n",
    "        #Iterate through all states to compute state values\n",
    "        for j in range(1,len(state_vals)-1):\n",
    "            win_prob = ph if policy[j-1] == 0 else pl\n",
    "            reward = -ch if policy[j-1] == 0 else -cl\n",
    "            if j == 5: #Special condition if a win from this state leads to victory\n",
    "                state_vals[j] = win_prob*(reward + R + gamma * v_old[j+1]) + (1-win_prob)*(reward + gamma * v_old[j-1])\n",
    "            else:\n",
    "                state_vals[j] = win_prob*(gamma*v_old[j+1]+reward) + (1-win_prob)*(gamma*v_old[j-1]+reward)\n",
    "            delta = np.maximum(delta, np.abs(v_old[j] - state_vals[j])) #Update delta\n",
    "        if delta < theta: #Determine whether termination criteria is met\n",
    "            termination = True\n",
    "\n",
    "    return state_vals\n",
    "\n",
    "import itertools\n",
    "policies = list(itertools.product(actions, repeat=5)) #generate all possible deterministic policies\n",
    "state_values = [iter_policy_eval(p) for p in policies] #compute state values for each policy\n",
    "pol_number = np.argmax([val[3] for val in state_values]) #retrieve policy with highest initial state value \n",
    "pol_value_init = np.max([val[3] for val in state_values]) #retrieve the corresponding value from this policy\n",
    "print('Optimal policy is {}'.format(policies[pol_number]))\n",
    "print('Initial state\\'s value under this policy is {}\\n'.format(pol_value_init))\n",
    "#Plot initial state values for all 32 policies\n",
    "labels = [str(p) for p in policies]\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "bars = plt.bar(range(32),[val[3] for val in state_values], label=policies)\n",
    "bars[28].set_color('red')\n",
    "plt.xlabel('Policy $\\pi$')\n",
    "plt.ylabel('Initial State Value $V_{\\pi}(0)$')\n",
    "# ax.set_xticklabels(labels)\n",
    "plt.xticks(np.arange(32),rotation=90, labels=labels)\n",
    "plt.title('Initial State Values for all 32 Deterministic Policies');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Monte Carlo Off-Policy Evaluation\n",
    "\n",
    "Using Monte Carlo methods, we can generate sample \"episodes\" of the game, where each episode consists of one full play-through of the game. From this sample, we can estimate the action-values on an episode-by-episode basis; however, because we have to wait for the conclusion of each episode to update our estimates, this method can suffer from high variance and thus require a very large number of episodes to converge to accurate action-values.\n",
    "\n",
    "Implementing Monte Carlo off-policy evaluation, I found that even with 500,000 episodes, my action values fluctuated somewhat significantly each time, as does the optimal policy. This is possibly because 500,000 episodes is not enough to lead to convergence, or it could be because the win probabilities $p_{High}=0.55$ and $p_{Low}=0.45$ are too close together, making it difficult to distinguish between the actions due to random chance. To compensate for this instability, I treated each run of 500,000 episodes as a simulation and ran the simulation 10 times. Averaging these results, I obtain the following action-values, which I found to be more robust to random variation:\n",
    "\n",
    "\n",
    "| State | Actions | |\n",
    "| --------- | ----------- | ---------- |\n",
    "|          | High Effort | Low Effort |\n",
    "| d = -3 | 0 | 0 |\n",
    "| d = -2 | 27.52 | **29.81** |\n",
    "| d = -1 | 68.44|**135.70** |\n",
    "| d = 0 | **271.22**|223.07 |\n",
    "| d = 1 |**491.56** | 468.64|\n",
    "| d = 2 |**751.88**|661.14|\n",
    "| d = 3 | 0 | 0 |\n",
    "\n",
    "As we can see from the bolded actions values (which are optimal given each state), the target policy specified below is not the optimal one. The optimal policy according to this MC estimation method is to invest low effort when losing and high effort when tied or winning. Reassuringly, this is very similar to the optimal policy found in part I using the initial state values (the only difference is the suggested action when $d=0$). In fact, the plot in part I shows that this policy of $[low, low, high, high, high]$ ranks in the top 5 in terms of initial state value. This gives us greater confidence that our results here make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target policy here is to always choose high effort unless winning by 2 rounds\n",
    "def target_policy(d):\n",
    "    return 0 if d <= 1 else 1\n",
    "#Random behavioral policy\n",
    "def behavior_policy():\n",
    "    return np.random.binomial(1, 0.5)\n",
    "\n",
    "env = tpg1.two_player_game1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_offpolicy_eval(target_policy, gamma=1, n_episodes=500000):\n",
    "    Q = np.zeros((7,2))\n",
    "    C = np.zeros_like(Q)\n",
    "    for ep in range(n_episodes):    \n",
    "        done = False\n",
    "        env.reset()\n",
    "        history = {'states':[0], 'actions':[], 'rewards':[]}\n",
    "        #Generate data according to behavior policy\n",
    "        while not done:\n",
    "            action = behavior_policy()\n",
    "            state, reward, done = env.step(action)\n",
    "            history['states'].append(state)\n",
    "            history['actions'].append(action)\n",
    "            history['rewards'].append(reward)\n",
    "            G=0\n",
    "            W=1\n",
    "        #Evaluate target policy\n",
    "        for i in range(len(history['actions'])-1, -1, -1):\n",
    "            state, action, reward = history['states'][i], history['actions'][i], history['rewards'][i]\n",
    "            G = gamma*G + reward \n",
    "            C[state+3][action] += W\n",
    "            #Update action values\n",
    "            Q[state+3][action] = Q[state+3][action] + (W/C[state+3][action])*(G-Q[state+3][action])\n",
    "            #Depending on state, target policy always or never takes the given action (deterministic)\n",
    "            target_prob = 1 if action == target_policy(state) else 0 \n",
    "            behavior_prob = 0.5 #Always 0.5 prob of choosing either action, regardless of state\n",
    "            #Update importance weights\n",
    "            W = W*target_prob/behavior_prob\n",
    "            #Exit loop if weight = 0\n",
    "            if W == 0:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ],\n",
       "       [ 27.51706568,  29.80957067],\n",
       "       [ 68.44108131, 135.69912484],\n",
       "       [271.22456805, 223.07100736],\n",
       "       [491.56175508, 468.6396233 ],\n",
       "       [751.87924601, 661.13557219],\n",
       "       [  0.        ,   0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conduct 10 runs of 500,000 episodes each\n",
    "Q_avg = np.zeros((7,2))\n",
    "runs = 10\n",
    "\n",
    "np.random.seed(1)\n",
    "for _ in range(runs):\n",
    "    Q_avg += MC_offpolicy_eval(target_policy)\n",
    "#Average action-values of the 10 runs\n",
    "Q_avg /= runs\n",
    "Q_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Monte Carlo Off-Policy Control\n",
    "\n",
    "Monte Carlo control is very similar to evaluation, with an extra step. After generating data according to a random behavior policy, the algorithm updates the action-values, just as done above. But control introduces a policy improvement element, in which the target policy greedily chooses the most valuable action at every step. This policy improvement procedure is guaranteed to yield a policy that is at least as good if not better than the existing target policy, and it allows for iterative improvement of the target policy. \n",
    "\n",
    "Again, for the same reasons as above, my results are subject to random variation, but now there is an additional reason: if the action taken by the behavior policy is different than the one the target policy would have taken at that step, then the episode ends. This can lead to some extremely short episodes, which introduces even more variance into the results. I omit the 10-run simulation here because the goal of off-policy control is to return a policy rather than a set of values that can be averaged together, but I want to note that this algorithm likely needs more episodes or more separation between the actions' win probabilities.\n",
    "\n",
    "We can see that through off-policy control, we obtain an optimal target policy of $[low, high, high, high, low]$. Some of the pairs of action values (e.g. for $d=0$) are extremely similar, demonstrating how different runs of the algorithms can yield different optimal policies. The action values do make sense, gradually increasing as the player gets closer to winning the game:\n",
    "\n",
    "| State | Actions | |\n",
    "| --------- | ----------- | ---------- |\n",
    "|          | High Effort | Low Effort |\n",
    "| d = -3 | 0 | 0 |\n",
    "| d = -2 | 47.06 | **53.53** |\n",
    "| d = -1 | **177.93**|166.00 |\n",
    "| d = 0 | **347.96**|344.97 |\n",
    "| d = 1 |**585.16** | 556.86|\n",
    "| d = 2 |744.83|**797.98**|\n",
    "| d = 3 | 0 | 0 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_offpolicy_ctrl(gamma=1, n_episodes=500000):\n",
    "    Q = np.zeros((7,2))\n",
    "    C = np.zeros_like(Q)\n",
    "    target_policy = np.zeros(7) #Arbitrary initialization of target policy\n",
    "    for ep in range(n_episodes):    \n",
    "        done = False\n",
    "        env.reset()\n",
    "        history = {'states':[0], 'actions':[], 'rewards':[]}\n",
    "        #Generate data according to behavior policy\n",
    "        while not done:\n",
    "            action = behavior_policy()\n",
    "            state, reward, done = env.step(action)\n",
    "            history['states'].append(state)\n",
    "            history['actions'].append(action)\n",
    "            history['rewards'].append(reward)\n",
    "            G=0\n",
    "            W=1\n",
    "        #Evaluate target policy\n",
    "        for i in range(len(history['actions'])-1, -1, -1):\n",
    "            state, action, reward = history['states'][i], history['actions'][i], history['rewards'][i]\n",
    "            G = gamma*G + reward\n",
    "            C[state+3][action] += W\n",
    "            Q[state+3][action] = Q[state+3][action] + (W/C[state+3][action])*(G-Q[state+3][action])\n",
    "            #Improve target policy greedily\n",
    "            target_policy[state+3] = np.argmax(Q[state+3])\n",
    "            behavior_prob = 0.5 #Always 0.5 prob of choosing either action, regardless of state\n",
    "            W = W*1/behavior_prob\n",
    "            if target_policy[state+3] != action:\n",
    "                break\n",
    "    return target_policy[1:6], Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 1.]),\n",
       " array([[  0.        ,   0.        ],\n",
       "        [ 47.05698557,  53.52835735],\n",
       "        [177.92650552, 166.00457419],\n",
       "        [347.95646706, 344.96646303],\n",
       "        [585.16292104, 556.86255089],\n",
       "        [744.82550574, 797.9760042 ],\n",
       "        [  0.        ,   0.        ]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "MC_offpolicy_ctrl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II\n",
    "## 1.) Q-Learning algorithm\n",
    "We move now to Temporal Difference (TD) methods. The first is Q-learning, an off-policy TD control method. In this implementation, the behavior policy is $\\epsilon$-greedy, while the the target policy used for learning is greedy. Just as with off-policy MC control, the behavior policy determines which states and actions are visited, and so we need it to be exploring the entire state-action space, which is a fair assumption given $\\epsilon=0.1$ over 500,000 episodes.\n",
    "\n",
    "Q-learning yields an optimal policy of $[low, low, low, high, high]$, which is identical to the optimal policy obtained through iterative evaluation in part I above. The results here are also stabler than the Monte Carlo results; if I rerun the simulation I get the same optimal policy. The action values obtained from Q-learning are:\n",
    "\n",
    "\n",
    "| State | Actions | |\n",
    "| --------- | ----------- | ---------- |\n",
    "|          | High Effort | Low Effort |\n",
    "| d = -3 | 0 | 0 |\n",
    "| d = -2 | 29.54 | **54.25** |\n",
    "| d = -1 | 132.02|**148.67** |\n",
    "| d = 0 | 274.65|**289.74** |\n",
    "| d = 1 |**469.42** | 461.88|\n",
    "| d = 2 |**710.84**|691.73|\n",
    "| d = 3 | 0 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(q_values, eps=0.1):\n",
    "    if np.random.binomial(1, eps) == 1:\n",
    "        return np.random.choice(np.arange(len(q_values)))\n",
    "    else:\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "env = tpg1.two_player_game1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(n_episodes=500000, eta=0.001, gamma=1):\n",
    "    Q = np.zeros((7,2))\n",
    "    rewards = np.zeros(n_episodes)\n",
    "    for ep in range(n_episodes):\n",
    "        env.reset()\n",
    "        state = 0 #Initial state\n",
    "        done = False\n",
    "        while not done:\n",
    "            #Choose action given state\n",
    "            action = eps_greedy(Q[state+3])\n",
    "            next_state, reward, done = env.step(action)\n",
    "            #Updates\n",
    "            Q[state+3, action] = Q[state+3, action] + \\\n",
    "                                        eta*(reward + gamma * np.max(Q[next_state+3]) - Q[state+3,action])\n",
    "            state = next_state\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.        ]\n",
      " [ 29.5419813   54.25199292]\n",
      " [132.02000078 148.66549623]\n",
      " [274.6541277  289.74485752]\n",
      " [469.41962209 461.87879183]\n",
      " [710.83897043 691.73465477]\n",
      " [  0.           0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "q_vals = q_learning()\n",
    "print(q_vals)\n",
    "np.argmax(q_vals, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) SARSA algorithm\n",
    "\n",
    "The SARSA algorithm is very similar to Q-learning, utilizing for policy improvement the quintuple of values $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$ that give it its name. The key difference is that at each step, the $a_t$ and $a_{t+1}$ are chosen according to the same policy (in this case $\\epsilon$-greedy), so SARSA is an on-policy learning method. \n",
    "\n",
    "SARSA yields the same optimal policy as Q-learning of $[low, low, low, high, high]$, and again this result is robust to random variation if I rerun the algorithm. The action-values are also extremely similar to the ones obtained from Q-learning. Both of these results provide a nice demonstration of the advantages of TD methods, which can update step-by-step within an episode, compared to the episode-by-episode updating structure in MC methods. For a game such as this one with so much variation, TD methods are able to yield estimates that converge much faster and are subject to less variation. Here are the estimated action-values:\n",
    "\n",
    "| State | Actions | |\n",
    "| --------- | ----------- | ---------- |\n",
    "|          | High Effort | Low Effort |\n",
    "| d = -3 | 0 | 0 |\n",
    "| d = -2 | 26.04 | **53.82** |\n",
    "| d = -1 | 119.35|**141.19** |\n",
    "| d = 0 | 262.63|**271.05** |\n",
    "| d = 1 |**458.49** | 451.63|\n",
    "| d = 2 |**700.12**|679.58|\n",
    "| d = 3 | 0 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(n_episodes=500000, eta=0.001, gamma=1):\n",
    "    Q = np.zeros((7,2))\n",
    "    rewards = np.zeros(n_episodes)\n",
    "    for ep in range(n_episodes):\n",
    "        env.reset()\n",
    "        state = 0 #Initial state\n",
    "        done = False\n",
    "        action = eps_greedy(Q[state+3]) #Choose action according to eps greedy\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action = eps_greedy(Q[next_state+3]) #Next action chosen according to same eps greedy policy \n",
    "            #Updates\n",
    "            Q[state+3, action] = Q[state+3, action] + \\\n",
    "                                        eta*(reward + gamma * Q[next_state+3, next_action] - Q[state+3,action])\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.        ]\n",
      " [ 26.04403363  53.81995496]\n",
      " [119.35399459 141.19028847]\n",
      " [262.62822186 271.04579305]\n",
      " [458.49044489 451.6345784 ]\n",
      " [700.12340078 679.5771637 ]\n",
      " [  0.           0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "q_vals = sarsa()\n",
    "print(q_vals)\n",
    "np.argmax(q_vals, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) TD($\\lambda$) algorithm\n",
    "\n",
    "Finally, we use TD($\\lambda$) to evaluate the performance of an agent that uses a random policy to play the game. Unlike Q-learning and SARSA, TD($\\lambda$) incorporates the use of eligibility traces, which is essentially a way to track which states are \"eligible\" for learning, based on the frequencies of visits to those states. **Note**: The older edition of Barto and Sutton's *Reinforcement Learning: An Introduction* fails to include in the pseudocode for TD($\\lambda$) and SARSA($\\lambda$) that the eligibility traces should be reset at the start of each episode, but I do so here because this seems to have been a typo (and it doesn't make sense for state visits in episode *t* to be carried over to episode $t+1$). \n",
    "\n",
    "In the TD($\\lambda$) algorithm, $\\lambda \\in [0,1] $ is a hyperparameter that allows the algorithm to bridge the gap between then MC and TD methods described above. If $\\lambda=0$, this is equivalent to a TD method that updates after every step within an episode (i.e. 1-step backup), and if $\\lambda=1$, this is equivalent to a MC algorithm that only updates after the completion of the episode (i.e. full backup).\n",
    "\n",
    "Here are the estimated values from this random policy according to TD($\\lambda=0.9$):\n",
    "\n",
    "| State | Value |\n",
    "| ----- | ----- |\n",
    "| d = -3 | 0|\n",
    "| d = -2 | 14.16 |\n",
    "| d = -1 | 91.39 |\n",
    "| d = 0 | 227.23 |\n",
    "| d = 1 | 422.03 |\n",
    "| d = 2 | 680.18|\n",
    "| d = 3 | 0 |\n",
    "\n",
    "Interestingly, the initial state value ($d=0$) is 227.23. Below, I reproduce the plot from part I that shows the estimated initial state values for all deterministic policies, with a red dashed line marking the TD($\\lambda$) estimate for this random policy. We can see that the random policy outperforms just about half of the deterministic policies (15 out of 32) and underperforms the remaining policies, which is a nice sanity check of these results since this is exactly what we should expect from a completely random policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda(n_episodes=500000, gamma=1, lam=0.9, eta=0.0001):\n",
    "    state_values = np.zeros(7)\n",
    "#     elig = np.zeros(7)\n",
    "    for ep in range(n_episodes):\n",
    "        elig = np.zeros(7)\n",
    "        env.reset()\n",
    "        state = 0 #Initial state\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.random.binomial(1, 0.5) #Random policy\n",
    "            next_state, reward, done = env.step(action)\n",
    "            delta = reward + gamma*(state_values[next_state+3] - state_values[state+3])\n",
    "            elig[state+3] += 1\n",
    "            #Update possible state values for s''\n",
    "            state_values = state_values + eta*elig*delta\n",
    "            elig *= gamma*lam\n",
    "            state = next_state\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.          14.15949244  91.38867395 227.22534913 422.02638058\n",
      " 680.18444083   0.        ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tdlam_state_vals = td_lambda()\n",
    "print(tdlam_state_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAG+CAYAAABYnShSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzwUlEQVR4nO3deZhkZXn///dHUEFFAUFFFlEBFZegYXGJiQvu+EXihkYFo0GNGs3X/AyafBPU4JK4xyWiGHBFFFFUXBAV3BEQFQQEEWQE2RFQQJb798c5IzU9PTNdw+nup7rer+vqq6vPqbrPp06d6bn7qeecSlUhSZIk6ea7xWIHkCRJkpYKm2tJkiRpIDbXkiRJ0kBsriVJkqSB2FxLkiRJA7G5liRJkgZicy0tQUm+nGSv1az/nyT/b461vpXkhcOla1OSs5PsusDbfEmSC5JcleSO87ytvZN8Z+TnSrLNfG5zUiX5myRfG/q+szz2lCSPWJvHrsW2tu5f83X7n1f7O2LkcQuWUVoqbK6lCTFO81dVT6iqg/vHrdBU9etfXFVvGCDThkk+nOS3Sa5M8osk/zyyfqwG7uY08kk+kOQjsyx/QJJrk2y8NnXnS5JbAm8HHltVt6uqSxY703JJ/jPJuUmuSHJOkn8ZWbddks8nuSjJpUm+muReq6l1UJI/9sfHlUlOTvKmJHcYI8+C/iFQVR+vqscOed9+P/zHjMfet6q+NU62kSb5qv7r7CT7jlOj3/affkes4X5jZ5Smnc21pJvjHcDtgPsAdwD+D/DLRcpyEPDXSW47Y/nzgC9W1aULH2m17gysB5wy7gPTmc/f3wcC966q2wMPBZ6d5K/7dRsCRwD3onsOxwGfX0O9/6yqDYBNgecDDwa+O8trNbjlI7VL0IZVdTvgWcC/JXn8YgeS1LG5libQ8tHoJG9NclmSXyV5wsj6byV5YZL7AP8DPKQf5bq8X/+nUbQkGyX5Yj8SeVl/e4s5RtkJ+ERVXVZVN1bVaVX1mb7usf19ftJv+5mr21aS/YGHA+/p7/+efvm9kxzVj5KenuQZswWpqu8DvwGeOrIf1gGeDRyc5J5JvpHkkiQXJ/l4kg1XsX9XGGVM8ogky0Z+vmuSw/rn8ask/zCybuckx/ejvhckefss9bcDTu9/vDzJN/rlD03yoyS/678/dOQx30qyf5LvAn8A7jFL3X2T/LIfIf55kj1me35rUlWnV9XvRxbdCGzTrzuuqg6sqkur6jq6P7DulTlMa6mqa6rqR3R/hN2RrtFenv1vk5zaHxdfTXK3fvlKx1G/fLckJyW5PMn3kjxgpNbZSf45yU+B3yfZph/tfX66EfnLkrw4yU5JftrXeM/I42ebQvPiJGf0j31vksy8b/9HzzuSXNi/hj9Ncr8k+wB/A7y6fw5fGMm5a397nSSvHXn9Tkiy5Rz26ffp/kC7X5JbJPnXdO82XJjkI1nFOwSZ8S5Rkr/r9//yY+dBs2S8xcgxdkmSQ9O/I5RkvSQf65df3h+/d15TfmkpsrmWJtcudA3aJsB/Agcu/w9/uao6FXgx8P1+6sGGs9S5BfC/wN2ArYCrgffMcr/Z/ADYv29atp2x7b/sb/5Zv+1PrW5bVfUvwLeBl/X3f1m6kc2jgE8Ad6IbpXtfkvuuIs9H6Eaql9sVuCXwZSDAm4C70o20bwnsN8fn+SfpRoy/APwE2Bx4NPDKJI/r7/Iu4F39qO89gUNn1qiqXwDLn8OGVfWovkn5EvBuusbz7cCXZjStzwX2ATYAzpkl3i/p/kC5A/A64GNJNhv3OfbPc98kVwHLgNvSvQaz+Uvgt+NMa6mqK+le14f323oK8Frgr+lGt78NfLK/70rHUd/4fRh4Ed2++gBwRJJbj2zmWcCT6Ebar++X7QJsCzwTeCfwL3THyH2BZyT5q9XE3o3uj8k/A54BPG6W+zyWbn9s12/3mcAlVXUA8HG6EfzbVdWTZ3ns/+0zPxG4PfC3dH9ErVLfzD+sz/9jYO/+65F0f3zdjjn8W07ydLp/C8/rt/1/gNlez38AngL8Fd2/o8uA9/br9qI77rake01eTPfvW5o6NtfS5Dqnqj5YVTcABwOb0b1NP5aquqSqDquqP/RNz/50/3nOxcvpmoaXAT9PcmZGRtAH2NZuwNlV9b9VdX1VnQgcBjxtFff/KPBXuWnk/Xl0I+vXVdWZVXVUVV1bVRfRNa9zfZ6jdgI2rarXV9Ufq+os4IPAnv3664BtkmxSVVdV1Q/mWPdJwBlV9dH+uX4SOA0YbcQOqqpT+vXXzSxQVZ+uqvP6dxE+BZwB7LwWz5GqejNdE/8guv36u5n36ffze+kaw3GdByyfB/8i4E1VdWpVXQ+8Edhh+ej1LP4O+EBV/bCqbujnDl9LN91kuXdX1blVNdrgvaEfPf8a8Hvgk1V1YVX9hq6hf+Bq8r65qi6vql8D3wR2mOU+19Hts3sD6Z/P+aupOeqFwL/27xpUVf1kDX+wXAxcCnwI2LeqjqYbHX97VZ1VVVcBrwH2zJqnxryQrvH/Ub/tM6tqtj/eXgT8S1Utq6pr6Rryp/X1r6NrqrfpX5MTquqKOT53aUmxuZYm12+X36iq5SNctxu3SJLbpDsZ8JwkVwDHAhumm1KxWlV1dVW9sar+nO4/1kOBT2cVJw+uxbbuBuzSv818ebppLX8D3GUVeX7d13xOktvRjbItP7HzTkkOSfKbftsfoxv1H9fdgLvOyPRabvrD5gV0I5en9W+N7zbHundl5dHoc+hGx5c7d3UFkjwvN02VuBy4H2v3HAHoG60f041Avm7GtjYFvga8r/9DYFyb0zWH0O3Td43kvpTunYbNV/HYuwGvmvEabEm3D5ebbV9dMHL76ll+Xt2/n9+O3P7DbPetqm/QjRS/F7ggyQFJbr+amqO2ZLzzFTapqo2q6j5V9e5+2cxj6BxgXdb8R/dct3034PCRfX4qcENf/6PAV4FDkpyX7qTYW8796UhLh821tPTVGta/iu7ktF36qQzL34bPqh8yy0a6Uao30k0huPtabmtm1nOBY6pqw5Gv21XVS1YT5WC6EeunAr/qR7uhmxJSwAP6bT+HVT/H3wO3Gfl5tJk/t687mmmDqnoiQFWdUVXPopvG8hbgM5nbiXvn0TUvo7aim0e+3Cpfy36U94N07yLcsZ8CdDJjvo6rsC7dFJfl29qIrrE+oqr2H7dY/4fPrnSjxdDt0xfN2KfrV9X3VlHiXGD/Gfe/zYwmf03H/byoqnf3f2zel+6PrP9vjnnOZWQfr6WZx9BWdFNiLpj97mNv+1zgCTP2+3pV9Zv+3aHXVdX2dCfB7saKU7SkqWFzLS19FwBbJLnVKtZvQDdqd3k/4vzvcy2c5P+lOynsVknWA14BXM5NJ+tdwIon3q1pWzPv/0VguyTPTXLL/mundCdqrsphdCNxr6MftR7Z9lX9tjfnpqZnNicBT0yycZK7AK8cWXcccEW6E+bWT3ci2v2S7NTvk+ck2bSqbuz3BXSje2tyZP9cn51k3XQn7m3f74O5uC1dA3dRn+P5dCPXY+lPWntRupNPk2Rn4KXA0f3629ONUH63qsa6BFySWyf5c+BzdPN1/7df9T/Aa5bPpU9yh34e8HIzj4sPAi9Oskuf8bZJnpRkg3Gf75D6Y3OXfsT298A13PTaz3wOM30IeEOSbfvn9ICMf+3zTwL/mOTu/R8wbwQ+1U+1WZ0PAf+U5M/7bW+ziik5/0N3jsXyk003TbJ7f/uRSe7fvwt1Bd00kbkc99KSY3MtLX3foLuawG+TXDzL+ncC69PN4fwB8JUxahddg3Qx3ajZY4An9fM9oZuTeXD/NvIz5rCtd9HN4bwsybv7edmPpZvPfB7dW/NvAW7NKlR3lYvlDfbHR1a9jm7+8O/oThz87Gqe10fpTlg8m26E9lMj9W+gmwe9A/Cr/rl8iO5kLoDHA6ekOxnwXcCeVXXNara1vO4ldKN9r6I7mezVwG5VNdtrNtvjfw68Dfg+XSN3f+C7c3nsLPagmyZwJd30mf/uv5av2wl4fm661vJVSbZaTb1XJ7mSbrrHR4ATgIf2rxVVdTjd63pIP2XnZGB07v5+jBxHVXU83bzr99A16WfSnci32G5P1/hfRjcl4xLgrf26A4Ht++fwuVke+3a6aVVfo2tOD6T7tzKOD9Mdu8fSHZvX0J0XsVpV9Wm68x8+Qfeaf46b5sOPehfdZRi/1r+eP6A7SRS6d3c+02c/FTiG7tiRpk6qFuWdM0mSJGnJceRakiRJGojNtSRJkjQQm2tJkiRpIDbXkiRJ0kBsriVJkqSBrOkjUSfKJptsUltvvfVix5AkSdISd8IJJ1xcVZvOXL6kmuutt96a448/frFjSJIkaYlLcs5sy50WIkmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgNteSJEnSQGyuJUmSpIHYXEuSJEkDsbmWJEmSBmJzLUmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgNteSJEnSQGyuJUmSpIGsu9gBJEmSpsnW+35prR979pufNGASzQdHriVJkqSB2FxLkiRJA7G5liRJkgbinGtJkiQ5F3wgjlxLkiRJA7G5liRJkgZicy1JkiQNxOZakiRJGojNtSRJkjQQm2tJkiRpIDbXkiRJ0kBsriVJkqSB+CEykiRJE8oPfmmPI9eSJEnSQGyuJUmSpIHYXEuSJEkDcc61JEmSBjXNc8EduZYkSZIGYnMtSZIkDcTmWpIkSRqIzbUkSZI0EE9olCRJN9s0n8AmjXLkWpIkSRqIzbUkSZI0EJtrSZIkaSA215IkSdJAbK4lSZKkgdhcS5IkSQOxuZYkSZIGYnMtSZIkDWTBPkQmyZbAR4C7ADcCB1TVu5LsB/wdcFF/19dW1ZH9Y14DvAC4AfiHqvrqQuWVJEmTzw+30UJbyE9ovB54VVWdmGQD4IQkR/Xr3lFVbx29c5LtgT2B+wJ3Bb6eZLuqumEBM0uaEP4HKklqwYJNC6mq86vqxP72lcCpwOarecjuwCFVdW1V/Qo4E9h5/pNKkiRJa2dR5lwn2Rp4IPDDftHLkvw0yYeTbNQv2xw4d+Rhy5ilGU+yT5Ljkxx/0UUXzVwtSZIkLZgFb66T3A44DHhlVV0BvB+4J7ADcD7wtuV3neXhtdKCqgOqaseq2nHTTTedn9CSJEnSHCxoc53klnSN9cer6rMAVXVBVd1QVTcCH+SmqR/LgC1HHr4FcN5C5pUkSZLGsZBXCwlwIHBqVb19ZPlmVXV+/+MewMn97SOATyR5O90JjdsCxy1UXkmSPFFW0rgW8mohDwOeC/wsyUn9stcCz0qyA92Uj7OBFwFU1SlJDgV+TnelkZd6pRBJkiS1bMGa66r6DrPPoz5yNY/ZH9h/3kJJkiRJA1rIkWtJkqQ1cjqOJpnNtSTNExsESZo+i3Kda0mSJGkpsrmWJEmSBmJzLUmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgXopPkmbwEnpqncfo4nC/ay5srqWG+YtckqTJ4rQQSZIkaSA215IkSdJAbK4lSZKkgdhcS5IkSQPxhEZJmjKeKCtJ88eRa0mSJGkgjlxL0gRwtHny+RpK08GRa0mSJGkgNteSJEnSQJwWshAe8YiVlz3jGfD3fw9/+AM88Ykrr9977+7r4ovhaU9bef1LXgLPfCacey4897krr3/Vq+DJT4bTT4cXvWjl9f/6r7DrrnDSSfDKV668/o1vhIc+FL73PXjta1de/853wg47wNe/Dv/xHyuv/8AH4F73gi98Ad72tpXXf/SjsOWW8KlPwfvfv/L6z3wGNtkEDjqo+5rpyCPhNreB970PDj105fXf+lb3/a1vhS9+ccV1668PX/5yd/sNb4Cjj15x/R3vCIcd1t1+zWvg+99fcf0WW8DHPtbdfuUru304arvt4IADutv77AO/+MWK63fYodt/AM95DixbtuL6hzwE3vQmAN5/+BvZ6OorVlj93bv9Gf/9sGcBcNCh/85611+7wvqj77kzH9zlr7sfpujYO+SsSwB47eNexll33IJHn/lD/u64w1d6+D/u9irOv/2m7HbqsTznx0d2C3/wXzfd4TOfAeBpP/s6T/vZ11d6/N5P349rbrkezznxS+x22rdXzrf87fu3vpVDPnHwCquuWffW7P2M1wHw8u9+koed85MV1l+2/u15yR79c55x7B1y1iWcv8Em/OOT/wmAf/v6AWx/4VkrPP6sjTfntY9/OQBv/Mp/c49Lf7Pi8xs59t7xhbey2ZUXr/D4Eze/N//5V3sDqz72oH9+T3gCXH31is99t93gn7p8S+XYW35cAbz+0fvw8zvfg4edfRIv/94hKz18pWNv9LgCNtt+75WPvdGn95TXcNlt7jD7sfeD/1rh994hn1j59+aez34zAH/3w8/y6F8et8K60WNv3n7vbbw7MOPY6/38Tvfg9bvuA8x+7JHv/On3Hk99Kof8aMXfmzfn994hZ13CF+/9cD72oCex3nXXcNCn92Omz9x/Vz5z/13Z6A+/4/2fe9NNK5a/hv2xt9kVF/GOL678f9oHd96Do7fZhXtcsow3fvU9K63/74fuCTzpT8fe6HEF8J9/uRcnbnEfHrTsVF597MErPX702OMRKx5Xh5x1ydr93ht9fv3/uWv7e2/5sTf6f+7y5zjO771XH3MQD/rNaSv+21nTsbf8//uGOHItSZIkDSRVtdgZBrPjjjvW8ccfv9gxpMF4AtTcDbmvhqrVYqahay11re73Fl/DVp9fi7VazNRyrVYlOaGqdpy53JFrSZIkaSDOuZY0lmkYjZAkaW05ci1JkiQNxJFrSZKm1M15Jwp8N0qajSPXkiRJ0kBsriVJkqSB2FxLkiRJA7G5liRJkgbiCY1acF7KbeG5zzUJPE4lLQWOXEuSJEkDsbmWJEmSBmJzLUmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgXopPkrTWvHyeJK3IkWtJkiRpIDbXkiRJ0kBsriVJkqSBOOd6AM45lCRJEjhyLUmSJA3G5lqSJEkaiNNCJC0ap1RJkpYaR64lSZKkgSxYc51kyyTfTHJqklOSvKJfvnGSo5Kc0X/faOQxr0lyZpLTkzxuobJKkiRJa2Mhp4VcD7yqqk5MsgFwQpKjgL2Bo6vqzUn2BfYF/jnJ9sCewH2BuwJfT7JdVd2wgJk1JZyeIEmShrBgI9dVdX5VndjfvhI4Fdgc2B04uL/bwcBT+tu7A4dU1bVV9SvgTGDnhcorSZIkjWtR5lwn2Rp4IPBD4M5VdT50DThwp/5umwPnjjxsWb9MkiRJatLYzXWS2yZZZ203mOR2wGHAK6vqitXddZZlNUu9fZIcn+T4iy66aG1jSZIkSTfbGpvrJLdI8uwkX0pyIXAacH5/UuJ/Jdl2rhtLcku6xvrjVfXZfvEFSTbr128GXNgvXwZsOfLwLYDzZtasqgOqaseq2nHTTTedaxRJkiRpcHM5ofGbwNeB1wAnV9WN0F3lA3gk8OYkh1fVx1ZXJEmAA4FTq+rtI6uOAPYC3tx///zI8k8keTvdCY3bAsfN9Ylp6fMkREmS1Jq5NNe7VtV1MxdW1aV0o9CH9SPSa/Iw4LnAz5Kc1C97LV1TfWiSFwC/Bp7e1z8lyaHAz+muNPJSrxQiSVoT//CWtJjW2FxX1XVJ7k139Y7N6eY9nwccUVWnLr/PHOp8h9nnUQM8ehWP2R/Yf021JUmSpBbMZc71PwOH0DXGxwE/6m9/sr8utSRJkiTmNi3kBcB9Z45O93OhT6Gb1iFJkiRNvblciu9GuhMKZ9qsXydJkiSJuY1cvxI4OskZ3PShLlsB2wAvm6dckiRJ0sSZywmNX0myHd1Hj29ON996GfAjr94hSZIk3WSNzXWS9Ne2/sEa7rPSpydKkiRJ02Quc66/meTlSbYaXZjkVkkeleRgug9/kSRJkqbaXOZcPx74W7pL790duBxYn64x/xrwjqo6ab4CSpIkSZNiLnOurwHeB7yv/yTGTYCrq+ryec6mm8lPKZMkSVpYc5kW8idVdV1VnQ/caZ7ySJIkSRNrLtNCZvOVJJ8GfgJsVFXvHTCTJEmSNJHGGrke8V3gXcCVwFXDxZEkSZIm19o21w+hO8kR4BsDZZEkSZIm2lyuc33vqjptxuKXAmcDfwbsA/y/4aNNJ09CnHy+hpIkTa+5zLk+Msm3gP2q6tcAVfXVft3pwKHzlE2SJEmaKHOZFnJv4MfAMUnemWTTec4kSZIkTaQ1NtdV9ceq+m/gPsAy4IdJXp9kg3lPJ0mSJE2QOZ/QWFXXVNVbgfsD1wAnJvmneUsmSZIkTZg5N9dJtk7yeOCFwFZ0l+F743wFkyRJkibNXK4W8lNgC+DXwGnAqXSX33sv3QmNkiRJkpjb1UL2AM6qqprvMJIkSdIkW2NzXVW/XIggkiRJ0qRb209olCRJkjSDzbUkSZI0kHGuFpIkz0nyb/3PWyXZef6iSZIkSZNlnJHr9wEPAZ7V/3wl3RVDJEmSJDG3q4Ust0tVPSjJjwGq6rIkt5qnXJIkSdLEGWfk+rok6wAFkGRT4MZ5SSVJkiRNoHGa63cDhwN3TrI/8B38hEZJkiTpT+Y8LaSqPp7kBODR/aKnVNWp8xNLkiRJmjxzbq6XXyVkxNOTUFWvHziTJEmSNJHGOaHx9yO31wN2Axy5liRJknrjTAt52+jPSd4KHDF4IkmSJGlC3ZxPaLwNcI+hgkiSJEmTbpw51z+jvwwfsA6wKeB8a0mSJKk3zpzr3UZuXw9cUFXXD5xHjdp63y+t9WPPfvOTBkwiSZLUrnHmXJ8zn0EkSZKkSbfG5jrJldw0HWSFVUBV1e0HTyVJkiRNoDU211W1wUIEkSRJkibdOHOuSbIRsC3dda4BqKpjhw4lSZIkTaJxrhbyQuAVwBbAScCDge8Dj5qXZJIkSdKEGec6168AdgLOqapHAg8ELpqXVJIkSdIEGqe5vqaqrgFIcuuqOg241/zEkiRJkibPOHOulyXZEPgccFSSy4Dz5iOUJEmSNInGuc71Hv3N/ZJ8E7gD8JV5SSVJkiRNoDVOC0nyniQPHV1WVcdU1RFV9cf5iyZJkiRNlrnMuT4DeFuSs5O8JckOa7OhJB9OcmGSk0eW7ZfkN0lO6r+eOLLuNUnOTHJ6ksetzTYlSZKkhbTG5rqq3lVVDwH+CrgU+N8kpyb5tyTbjbGtg4DHz7L8HVW1Q/91JECS7YE9gfv2j3lfknXG2JYkSZK04OZ8tZCqOqeq3lJVDwSeDewBnDrG44+la87nYnfgkKq6tqp+BZwJ7DzXbUmSJEmLYc7NdZJbJnlyko8DXwZ+ATx1gAwvS/LTftrIRv2yzYFzR+6zrF8mSZIkNWsuJzQ+JsmH6RrcfYAjgXtW1TOr6nM3c/vvB+4J7ACcD7xt+WZnuW+tIt8+SY5PcvxFF/mZNpIkSVo8cxm5fi3dx5zfp6qeXFUfr6rfD7Hxqrqgqm6oqhuBD3LT1I9lwJYjd92CVVxTu6oOqKodq2rHTTfddIhYkiRJ0lqZywmNj6yqD1bVXOdLz1mSzUZ+3ANYfiWRI4A9k9w6yd2BbYHjht6+JEmSNKRxPqHxZknySeARwCZJlgH/Djyiv7RfAWcDLwKoqlOSHAr8HLgeeGlV3bBQWSVJkqS1sWDNdVU9a5bFB67m/vsD+89fIkmSJGlY41wtJEmek+Tf+p+3SuLl8SRJkqTenJtr4H3AQ4DlI9BXAu8dPJEkSZI0ocaZFrJLVT0oyY8BquqyJLeap1ySJEnSxBln5Pq6/iPICyDJpsCN85JKkiRJmkDjNNfvBg4H7pRkf+A7wJvmJZUkSZI0geY8LaSqPp7kBODRdJ+g+JSqOnXekkmSJEkTZs7NdZK3VNU/A6fNskySJEmaeuNMC3nMLMueMFQQSZIkadKtceQ6yUuAvwfukeSnI6s2AL43X8EkSZKkSTOXaSGfAL5Md/LiviPLr6yqS+cllSRJkjSB1thcV9XvgN8Bz0qyEbAtsB5AEqrq2PmNKEmSJE2GcU5ofCHwCmAL4CTgwcD3gUfNSzJJkiRpwoxzQuMrgJ2Ac6rqkcADgYvmJZUkSZI0gcZprq+pqmsAkty6qk4D7jU/sSRJkqTJM+dpIcCyJBsCnwOOSnIZcN58hJIkSZIm0Tif0LhHf3O/JN8E7kB3FRFJkiRJjDEtJMlblt+uqmOq6gjgP+YllSRJkjSB/IRGSZIkaSA39xMavztfwSRJkqRJ4yc0SpIkSQMZ6xMa5z+OJEmSNLnWOOc6yU5J7jLy8/OSfD7Ju5NsPL/xJEmSpMkxlxMaPwD8ESDJXwJvBj5CN5p9wPxFkyRJkibLXOZcrzMyt/qZwAFVdRhwWJKT5i2ZJEmSNGHmMnK9TpLlTfijgW+MrBvnEx4lSZKkJW0uzfEngWOSXAxcDXwbIMk2dFNDJEmSJDG3q4Xsn+RoYDPga1VV/apbAC+fz3CSJEnSJJnTtI6q+sEsy34xfBxJkiRpco3z8eeSJEmSVsPmWpIkSRqIzbUkSZI0kDXOuU5yJVCzrQKqqm4/eCpJkiRpAs3laiEbLEQQSZIkadKN9SEwSTYCtgXWW76sqo4dOpQkSZI0iebcXCd5IfAKYAvgJODBwPeBR81LMkmSJGnCjHNC4yuAnYBzquqRwAOBi+YllSRJkjSBxmmur6mqawCS3LqqTgPuNT+xJEmSpMkzzpzrZUk2BD4HHJXkMuC8+QglSZIkTaI5N9dVtUd/c78k3wTuAHxlXlJJkiRJE2isq4UsV1XHDB1EkiRJmnRz+RCZ71TVX8zyYTJ+iIwkSZI0Yo0nNFbVX/Q3319Vtx/52gD4n/mNJ0mSJE2Oca4Wsussyx4/VBBJkiRp0s1lWshLgL8H7pnkpyOrNgC+N1/BJEmSpEkzlxMaPwF8GXgTsO/I8iur6tJ5SSVJkiRNoDU211X1O+B3wLPmP44kSZI0udY45zrJd/rvVya5YuTryiRXzHVDST6c5MIkJ48s2zjJUUnO6L9vNLLuNUnOTHJ6kseN+8QkSZKkhTbnq4VU1QYzrxYy5mX4DmLlEyD3BY6uqm2Bo/ufSbI9sCdw3/4x70uyzhjbkiRJkhbcOFcLuVmq6lhg5hzt3YGD+9sHA08ZWX5IVV1bVb8CzgR2XoickiRJ0tqa8yc0Jrk18FRg69HHVdXrb8b271xV5/d1zk9yp3755sAPRu63rF8mSZIkNWucjz//PN2JjScA185PnD/JLMtqlmUk2QfYB2Crrbaaz0ySJEnSao3TXG9RVUN/aMwFSTbrR603Ay7sly8DthzdNnDebAWq6gDgAIAdd9xx1gZckiRJWgjjzLn+XpL7D7z9I4C9+tt70Y2OL1++Z5JbJ7k7sC1w3MDbliRJkgY1zsj1XwDPT3IW3bSQAFVVD5jLg5N8EngEsEmSZcC/A28GDk3yAuDXwNPpip6S5FDg58D1wEur6oYxskqSJEkLbpzm+vH0DfXabKiqVvUhNI9exf33B/Zfm21JkiRJi2GNzXWSK5m9oV7eaI9zrWtJkiRpyZrLx59vsBBBJEmSpEm3YB8iI0mSJC11NteSJEnSQGyuJUmSpIHYXEuSJEkDsbmWJEmSBmJzLUmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgNteSJEnSQGyuJUmSpIHYXEuSJEkDsbmWJEmSBmJzLUmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgNteSJEnSQGyuJUmSpIHYXEuSJEkDsbmWJEmSBmJzLUmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgNteSJEnSQGyuJUmSpIHYXEuSJEkDsbmWJEmSBmJzLUmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgNteSJEnSQGyuJUmSpIHYXEuSJEkDsbmWJEmSBmJzLUmSJA3E5lqSJEkaiM21JEmSNBCba0mSJGkgNteSJEnSQGyuJUmSpIHYXEuSJEkDWXexAwAkORu4ErgBuL6qdkyyMfApYGvgbOAZVXXZYmWUJEmS1qSlketHVtUOVbVj//O+wNFVtS1wdP+zJEmS1KyWmuuZdgcO7m8fDDxl8aJIkiRJa9ZKc13A15KckGSfftmdq+p8gP77nRYtnSRJkjQHTcy5Bh5WVecluRNwVJLT5vrAvhnfB2Crrbaar3ySJEnSGjUxcl1V5/XfLwQOB3YGLkiyGUD//cJVPPaAqtqxqnbcdNNNFyqyJEmStJJFb66T3DbJBstvA48FTgaOAPbq77YX8PnFSShJkiTNTQvTQu4MHJ4EujyfqKqvJPkRcGiSFwC/Bp6+iBklSZKkNVr05rqqzgL+bJbllwCPXvhEkiRJ0tpZ9GkhkiRJ0lJhcy1JkiQNxOZakiRJGojNtSRJkjQQm2tJkiRpIDbXkiRJ0kBsriVJkqSB2FxLkiRJA7G5liRJkgZicy1JkiQNxOZakiRJGojNtSRJkjQQm2tJkiRpIDbXkiRJ0kBsriVJkqSB2FxLkiRJA7G5liRJkgZicy1JkiQNxOZakiRJGojNtSRJkjQQm2tJkiRpIDbXkiRJ0kBsriVJkqSB2FxLkiRJA7G5liRJkgZicy1JkiQNxOZakiRJGojNtSRJkjQQm2tJkiRpIDbXkiRJ0kBsriVJkqSB2FxLkiRJA7G5liRJkgZicy1JkiQNxOZakiRJGojNtSRJkjQQm2tJkiRpIDbXkiRJ0kBsriVJkqSB2FxLkiRJA7G5liRJkgZicy1JkiQNxOZakiRJGojNtSRJkjQQm2tJkiRpIDbXkiRJ0kCabq6TPD7J6UnOTLLvYueRJEmSVqfZ5jrJOsB7gScA2wPPSrL94qaSJEmSVq3Z5hrYGTizqs6qqj8ChwC7L3ImSZIkaZVabq43B84d+XlZv0ySJElqUqpqsTPMKsnTgcdV1Qv7n58L7FxVL59xv32Affof7wWcvqBB52YT4OLGarWYaRpqtZhpGmq1mKnVWi1mmoZaLWaahlotZpqGWkNmWkx3q6pNZy5cdzGSzNEyYMuRn7cAzpt5p6o6ADhgoUKtjSTHV9WOLdVqMdM01Gox0zTUajFTq7VazDQNtVrMNA21Wsw0DbWGzNSilqeF/AjYNsndk9wK2BM4YpEzSZIkSavU7Mh1VV2f5GXAV4F1gA9X1SmLHEuSJElapWaba4CqOhI4crFzDGDIaStD1Wox0zTUajHTNNRqMVOrtVrMNA21Wsw0DbVazDQNtZqezntzNXtCoyRJkjRpWp5zLUmSJE0Um2tJkiRpIDbXkiRJ0kCaPqFxEiXZgu6ygQ8H7gpcDZwMfAn4clXdaK3ha7WYqdVaLWaahlotZpqGWi1marVWi5mmoVaLmVquNQk8oXFASf6X7iPavwgcD1wIrAdsBzwS+HNg36o61lrD1WoxU6u1Wsw0DbVazDQNtVrM1GqtFjNNQ60WM7Vca2JUlV8DfQH3W8P6WwHbWGvYWi1marVWi5mmoVaLmaahVouZWq3VYqZpqNVippZrTcqXI9eSJEnSQDyhcUBJ7pDkzUlOS3JJ/3Vqv2zDAbfzZXNNbqZWc7WYaankajHTtOdqMVOruVrMtFRytZip5VyTwhMah3Uo8A3gEVX1W4AkdwH2Aj4NPGauhZI8aFWrgB3M1XamVnO1mGlKcrWYacnnajFTq7lazDQluVrM1HKuieC0kAElOb2q7jXuulXc/wbgGLqDb6YHV9X65mo3U6u5Wsw0DblazDQNuVrM1GquFjNNQ64WM7Wca1I4cj2sc5K8Gji4qi4ASHJnYG/g3DFrnQq8qKrOmLkiybi1lnquFjO1mqvFTNOQq8VM05CrxUyt5mox0zTkajFTy7kmgnOuh/VM4I7AMUkuTXIp8C1gY+AZY9baj1W/Pi83V/OZWs3VYqZpyNVipmnI1WKmVnO1mGkacrWYqeVcE8FpIZIkSdJAHLmWJEmSBmJzLUmSJA3E5lqSJEkaiM31AkiyY5LNrbWwtVrM1GqtFjNNQ60WM01DrRYztVqrxUzTUKvFTC3Xao0nNC6AJAcDDwB+UVXPtNbC1GoxU6u1Wsw0DbVazDQNtVrM1GqtFjNNQ60WM7VcqzU21wsoyQZVdaW1FrZWi5lardVipmmo1WKmaajVYqZWa7WYaRpqtZip5VqtsLkeWJIAOwObAwWcBxxXa7Gjh6y1mm3cu6pOW6xaQz1H99XC11nDNpbEvhq61irqu6/G28Yg+8t9Nb91pnVfrU0t99UwtVpicz2gJI8F3gecAfymX7wFsA3w91X1tcWotYbt/LqqtlqMWkM9R/eV+2rG/Sfq36H7ajxD7S/31fzVmeZ9NW4t99VwtVrix58P613ArlV19ujCJHcHjgTusxi1krx7VauADcfINGgthnuO7iv31ajm/h26rxbn2HJfua9mPKbFf4fuqyXI5npY6wLLZln+G+CWi1jr+cCrgGtnWfesRaw11HN0Xy18HVj6+2rIWu6r8Qz1HN1XC18Hlv6+GrKW+2oJsrke1oeBHyU5BDi3X7YlsCdw4CLW+hFwclV9b+aKJPstYq2hnqP7auHrwNLfV0PWcl+NZ6jn6L5a+Dqw9PfVkLXcV0uQc64HluQ+wO50JxOE7q/II6rq54tVK8nGwDVV9YdxM8xnrb7eUM/RfbXwdZb8vhqqlvtqcY4t99XC1xmpt2T31TzUcl8tMTbXkiRJ0kD8hEZJkiRpIDbXkiRJ0kBsriVJkqSBeLWQBZDkjcDvgA9V1SXWWphaLWZqtVaLmaahVouZpqFWi5lardVipmmo1WKmlmu1xpHrhXEccD3wDmstaK0WM7Vaq8VM01CrxUzTUKvFTK3WajHTNNRqMVPLtZri1UIkSZKkgTgtZGBJHgc8he4akwWcB3y+qr5irbG28W9V9fpW6rRQa6j93uqxMEnH1ZC1FjtTq69hi8f7arbR3HE1ZC2Pq/mvtYr6zR0LLddqiSPXA0ryTmA74CPc9BGkWwDPA86oqldYa87b+XVVbdVKncWuNdR+b/VYmLTjashaS+G4arXWNB9XQ9byuJrfWqvZRnPHQsu1WmJzPaAkv6iq7WZZHuAXVbWttVZ4zBWrWgWsX1VzemdlqDqN1xpkvzd8LDR3XA1Zq8VMfa1WX8MWj/dWX8MWj9HmjoVWa7V4LLRca1J4QuOwrkmy8yzLdwKusdZKLge2rarbz/jaADh/Eeq0XGuo/d7qsdDicTVkrRYzQbuvYYvH++W0+RoOVWvITC0eC63Wupz2joWWa02EJffXwiLbG3h/kg246W2iLYEr+nXWWtFHgLsBF8yy7hOLUKflWnszzH4fqk7LtVp8DVvMBO2+hkPVGjJTq69hi8fo3rR3LLRaq8VjoeVaE8FpIfMgyV3oTnAIsKyqfmstDWGo/d7qseBxtThafQ1bPN41dy0eCy3X0tJhcy1JkiQNxDnXkiRJ0kBsriVJkqSB2FxLkiRJA7G5XgBJTu2/XmathavVYqZWa7WYaRpqtZhpGmq1mKnVWi1mmoZaLWZquVZrvBTfAqiq+yS5I/Bgay1crRYzzVOtTYBdWqhjrcnPNFKr1ePd3w0LWKvF31fTUKvFTC3Xao1XC5knSTYGqqous9bC1Wox09C1pPnQ6vHu74aFr+XvK+nmcVrIgJJsleSQJBcBPwR+lOTCftnW1pqfWi1mGrrWGrbzs5bqWGtx6qxNrVaPd383LHytSft9NQ21WszUcq2WOC1kWJ8C3gn8TVXdAJBkHeDpwCGM97aateZeq8VMg9ZK8terWgXcZaHrWGtp7HcaPd4HrNViplZrNff7ahpqtZip5VqTwmkhA0pyRlVtO+46a928Wi1mmoda1wEfB2b7B/u0qtpgIetYa7xaLWbqa7V6vPu7YYFrtfj7ahpqtZip5VqTwuZ6QEkOAS4FDgbO7RdvCewFbFJVz7DW8LVazDQPtU4A9qqqk2dZd25VbbmQday1ZPZ7q8e7vxsWuFaLv6+moVaLmVquNSlsrgeU5FbAC4Ddgc3p3vI4F/gCcGBVXWut4Wu1mGkeaj0cOKeqfj3Luh2r6viFrGOtJbPfWz3e/d2wwLVa/H01DbVazNRyrUlhcy1JkiQNxKuFSJIkSQOxuZYkSZIGYnMtSZIkDcTmegEk2T3JIB/vaa3JztRqrRYzTUOtFjNNQ60WM7Vaq8VM01CrxUwt12qNHyKzMHYB7p9k3ap6grUWrFaLmVqt1WKmaajVYqZpqNViplZrtZhpGmq1mKnlWk3xaiGSJEnSQBy5HliSOwCPp7tOaAHnAV+tqsutNX+1WszUaq0WM01DrRYzrWEbj6mqo5ZqrRYztVqrxUzTUKvFTC3XaolzrgeU5HnAicAjgNsAtwUeCZzQr7PWPNRqMVOrtVrMNA21Wsw0Bwcu8VotZmq1VouZpqFWi5lartUMp4UMKMnpwC4zR4+SbAT8sKq2s9bwtVrM1GqtFjNNQ60WM/WPOWJVq4BHVdVtJ7lWi5lardVipmmo1WKmlmtNCqeFDCt0b9HOdGO/zlrzU6vFTK3WajHTNNRqMRPAw4HnAFfNso2dl0CtFjO1WqvFTNNQq8VMLdeaCDbXw9ofODHJ14Bz+2VbAY8B3mCteavVYqZWa7WYaRpqtZgJ4AfAH6rqmJkr+hHySa/VYqZWa7WYaRpqtZip5VoTwWkhA+vfmn0c3YlGAZbRnWh0mbXmr1aLmVqt1WKmaajVYiZJ0vBsrgeUJLWGHTqX+1hrce4zDbVazDQNtVrMNA21WszUaq0WM01DrRYztVxrUni1kGF9M8nLk2w1ujDJrZI8KsnBwF7WGrxWi5lardVipmmo1WKmaajVYqZWa7WYaRpqtZip5VoTwZHrASVZD/hb4G+AuwOXA+vT/RHzNeC9VXWStYat1WKmVmu1mGkaarWYaTW11gPWWQq1WszUaq0WM01DrRYztVxrUthcz5MktwQ2Aa6um/nBDtaa7Eyt1mox0zTUajHTNNRqMVOrtVrMNA21WszUcq2W2VxLkiRJA3HOtSRJkjQQm2tJkiRpIDbXkiRJ0kBsriVJkqSB2FxL0gRKckOSk5KcnOTTSW6zhvtf1X//3sIklKTpZHMtSZPp6qraoaruB/wRePFcHlRVD53fWJI03WyuJWnyfRvYBiDJ/+1Hs09O8sqZd1w+gt3ffl6Snyb5SZKP9svekOQVI/fZP8k/zKixTZKLkpzdj55fmuSXSW4/X09QkiaF17mWpAmU5Kqqul2SdYHDgK8AxwEHAQ8GAvwQeE5V/Xjk/su/3xf4LPCwqro4ycZVdWmSrYHPVtWDktwCOAPYuaoumbH9w4G3V9W3k3wLeHlV/Wxhnr0ktcuRa0maTOsnOQk4Hvg1cCDwF8DhVfX7qrqKrnl++Coe/yjgM1V1MUBVXdp/Pxu4JMkDgccCP57ZWPfuC5zc3743cPoQT0qSJt26ix1AkrRWrq6qHUYXJMkYjw+wqrcuPwTsDdwF+PBKD0zWB9arqsuSbAlcUlV/HGPbkrRkOXItSUvHscBTktwmyW2BPejmY8/maOAZSe4IkGTjkXWHA48HdgK+OstjtwdO7W/fZ+S2JE09R64laYmoqhOTHEQ39xrgQ1X141Xc95Qk+wPHJLkB+DHdaDVV9cck3wQur6obZnn46JSQq4EHJbl3VZ023LORpMnkCY2SpBX0JzKeCDy9qs5Y7DySNEmcFiJJ+pMk2wNnAkfbWEvS+By5liRJkgbiyLUkSZI0EJtrSZIkaSA215IkSdJAbK4lSZKkgdhcS5IkSQOxuZYkSZIGYnMtSZIkDcTmWpIkSRrI/w/1FBSDkiGPJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [str(p) for p in policies]\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "bars = plt.bar(range(32),[val[3] for val in state_values], label=policies)\n",
    "plt.xlabel('Policy $\\pi$')\n",
    "plt.ylabel('Initial State Value $V_{\\pi}(0)$')\n",
    "# ax.set_xticklabels(labels)\n",
    "plt.xticks(np.arange(32),rotation=90, labels=labels)\n",
    "plt.hlines(y=tdlam_state_vals[3], xmin =0, xmax = 32, color = 'red', linestyle='dashed')\n",
    "plt.title('Initial State Values for all 32 Deterministic Policies');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III\n",
    "## 1.) SARSA($\\lambda$) algorithm\n",
    "We now explore a new setup of the game that is very similar but with a slight wrinkle. Now the player can change her behavior depending on both the win counts and her energy level, leading to a total of $7*11=77$ possible states. I implement the SARSA($\\lambda$) algorithm, which is analogous to TD($\\lambda$) above, except that now the goal is to learn the action values, rather than evaluate the state values given a policy (i.e. control instead of prediction).\n",
    "\n",
    "Another slight change is that while the target policy is still $\\epsilon$-greedy, we have to add a constraint that accounts for the fact that if the player doesn't have enough energy remaining to invest high effort, her only option is to choose low effort.\n",
    "\n",
    "Because there are 154 action values, I don't put them all into a table, but the output is visible below. Each of the seven 11x2 arrays corresponds to a different value of $d$, where the rows represent the different energy levels $B$ and the columns are each of the two actions. As $d$ and $B$ increase, we see a steady increase in the action values, which makes sense as players closer to victory and with more energy have more lucrative actions at their disposal. I do present a table below with the optimal deterministic policy given all possible states. A few things to note on this policy:\n",
    "\n",
    "- The agent will almost always choose high effort, except when constrained to choose low effort. In fact, there are only two states in which the agent will choose low effort when it doesn't have to (bolded below).\n",
    "\n",
    "- This makes sense, considering how high the initial value of $B=10$ is. Most episodes of the game will last far fewer than 10 rounds, so an agent could feasibly choose high effort, which is associated with a better win probability, and only very rarely run out of energy.\n",
    "\n",
    "- This result is not very stable. If I rerun the algorithm (even over a much larger number of episodes), I still get a similar policy in terms of mostly all \"High\" actions, but it is not identical. This suggests that the algorithm hasn't converged, which could be due to the increased number of states to explore and the relative rarity of some of those states being visited - especially states where $B$ is very low.\n",
    "       \n",
    "| | B = 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| **d = -3** | High | High | High | High | High | High | High | High | High | High | High |\n",
    "| **-2** | Low | High | High | High | High | High | High | High | High | High | High |\n",
    "| **-1** | Low | **Low** | **Low** | High | High | High | High | High | High | High | High |\n",
    "| **0** | Low | High | High | High | High | High | High | High | High | High | High |\n",
    "| **1** | Low | High | High | High | High | High | High | High | High | High | High |\n",
    "| **2** | Low | High | High | High | High | High | High | High | High | High | High |\n",
    "| **3** | High | High | High | High | High | High | High | High | High | High | High |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_lim(q_values, B, ch, eps=0.1):\n",
    "    '''Typical eps-greedy policy, but must choose action 1 (low effort) if agent doesn't have enough energy'''\n",
    "    if B < ch:\n",
    "        return 1\n",
    "    elif np.random.binomial(1, eps) == 1:\n",
    "        return np.random.choice(np.arange(len(q_values)))\n",
    "    else:\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "env = tpg2.two_player_game2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambda(n_episodes=500000, gamma=0.9, lam=0.9, eta=0.001):\n",
    "    Q = np.zeros((7,11,2)) \n",
    "#     elig = np.zeros((7,11,2))\n",
    "    ch = env.get_ch() #Energy cost of high effort\n",
    "    for ep in range(n_episodes):\n",
    "        elig = np.zeros((7,11,2))\n",
    "        #Initialize state (both d and B) and action\n",
    "        d, B = env.reset()\n",
    "        action = eps_greedy_lim(Q[d+3][B], B, ch) #Apply constrained eps-greedy policy\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_d, next_B = next_state\n",
    "            next_action = eps_greedy_lim(Q[next_d+3][next_B], next_B, ch)\n",
    "            delta = reward + gamma*Q[next_d+3][next_B][next_action] - Q[d+3][B][action]\n",
    "            elig[d+3][B][action] +=1 \n",
    "            Q += eta*elig*delta\n",
    "            elig *= gamma*lam\n",
    "            d, B = next_state\n",
    "            action = next_action\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]]\n",
      "\n",
      " [[  0.          52.7692215 ]\n",
      "  [ 59.87996038  30.0271749 ]\n",
      "  [ 66.89870865  52.08056313]\n",
      "  [ 72.13947902  40.12434697]\n",
      "  [ 77.81992035  56.08561941]\n",
      "  [ 85.56124063  50.77670946]\n",
      "  [ 90.88667662  72.88805286]\n",
      "  [ 93.97212808  63.29249281]\n",
      "  [ 96.08375897  78.56541201]\n",
      "  [100.61059757  67.49102511]\n",
      "  [ 98.67301856  69.38246708]]\n",
      "\n",
      " [[  0.         117.03888762]\n",
      "  [108.10046067 132.60694382]\n",
      "  [120.55733947 140.09871727]\n",
      "  [169.30491905 141.80655124]\n",
      "  [175.38767062 135.09624987]\n",
      "  [182.31294892 166.13371957]\n",
      "  [191.51337113 166.08273735]\n",
      "  [190.32564484 181.43568629]\n",
      "  [206.13928852 173.77607811]\n",
      "  [200.63044118 180.4289073 ]\n",
      "  [203.77049941 182.39694968]]\n",
      "\n",
      " [[  0.         226.66554774]\n",
      "  [251.87639704 181.75889432]\n",
      "  [270.22172381 250.94162997]\n",
      "  [291.86087781 245.26917207]\n",
      "  [300.40900791 279.28254711]\n",
      "  [306.0393007  266.14950162]\n",
      "  [315.24786932 292.63277394]\n",
      "  [330.71627471 300.51842519]\n",
      "  [321.8111796  295.79643318]\n",
      "  [335.68369785 300.58941327]\n",
      "  [332.34025783 307.51801423]]\n",
      "\n",
      " [[  0.         383.37946704]\n",
      "  [419.25052593 352.53555254]\n",
      "  [463.07220767 360.44057008]\n",
      "  [469.6454022  417.67231511]\n",
      "  [479.84466479 405.10984852]\n",
      "  [491.45536866 449.23533746]\n",
      "  [490.26962367 440.76719358]\n",
      "  [488.65900407 451.92058271]\n",
      "  [507.94976538 471.65124574]\n",
      "  [497.49960953 470.20968094]\n",
      "  [507.02822638 455.19752027]]\n",
      "\n",
      " [[  0.         648.33643503]\n",
      "  [720.80333096 404.38881781]\n",
      "  [721.76659738 554.59552792]\n",
      "  [738.05110559 480.34047556]\n",
      "  [749.70860496 627.75852879]\n",
      "  [742.024684   566.2449234 ]\n",
      "  [750.35575197 671.77436947]\n",
      "  [766.06980584 638.23932213]\n",
      "  [752.30881842 697.41556917]\n",
      "  [753.89024074 684.78532922]\n",
      "  [755.12343577 616.80295434]]\n",
      "\n",
      " [[  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]\n",
      "  [  0.           0.        ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "sarsa_lam_vals = sarsa_lambda(n_episodes=500000)\n",
    "print(sarsa_lam_vals)\n",
    "np.argmax(sarsa_lam_vals, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV\n",
    "## 1.) \n",
    "The described game is completely random with no strategy. In each round, each player will win with probability $p=\\frac{1}{2}$. Determining player $X$'s win probability for $x=1$ and various values of $y$ just requires knowledge of basic probability:\n",
    "\n",
    "- If $x=1$ and $y=1$, the player who wins the next round will win the entire game, so $P(win_X|x=1, y=1)=\\frac{1}{2}$ \n",
    "\n",
    "- If $x=1$ and $y=2$, player $X$ must win each of the next two rounds consecutively in order to win the entire game, so $P(win_X|x=1,y=2)=(\\frac{1}{2})^2=\\frac{1}{4}$ \n",
    "\n",
    "- For any value of $y$, this same pattern holds that player $X$ must win the next $y$ games consecutively to win the game, so $P(win_X|x=1,y)=(\\frac{1}{2})^y$ \n",
    "\n",
    "## 2.)\n",
    "We now determine the win probability for the opposite scenario of $y=1$ and various values of $x$:\n",
    "\n",
    "- Again, $y=1$ and $x=1$, the player who wins the next round will win the entire game, so $P(win_X|y=1,x=1)=\\frac{1}{2}$ \n",
    "\n",
    "- If $y=1$ and $x=2$, then player $X$ must win in one of the next two rounds. Since each round occurs independently of any other, this scenario is equivalent to the probability of winning the first round *OR* losing the first round and winning the second. $$P(win_X|y=1,x=2) = \\frac{1}{2} + (\\frac{1}{2})^2 = \\frac{3}{4}$$\n",
    "\n",
    "- Continuing this pattern, for any value of $x$, player $X$ must win just one of the next $x$ rounds to win the game:\n",
    "$$P(win_X |y=1, x) = \\frac{1}{2} + (\\frac{1}{2})^2 + ... + (\\frac{1}{2})^x = \\sum_{i=1}^x\\frac{1}{2}^i$$\n",
    "\n",
    "## 3.)\n",
    "\n",
    "For any value of $x$ or $y$, there is a closed form solution to the problem that can be solved by realizing that the event of Player $X$ winning the game is a random variable that follows a Negative Binomial Distribution. From Player $X$'s standpoint, the chances of winning the game can be thought of as the probability of achieving $y$ successes before $x$ failures. The corresponding probability mass function is:\n",
    "$$P(win_X|x,y) = \\sum_{i=0}^{x-1} {y+i-1 \\choose y-1}(1-p)^ip^y,$$ \n",
    "where $p = 0.5$ in this case. Another way to think about this intuitively is the probability of player $Y$ winning no more than $x-1$ times (any more would exhaust all of $X$'s tokens) before player $X$ wins $y$ times. Using this formula, here is a table with the win probability values for Player $X$ for each set of $(x,y) \\in [1,10] x [1,10]$:\n",
    "\n",
    "\n",
    "|  | y = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |\n",
    "| -- | ----- | -- | -- | -- | -- | -- | -- | -- | -- | -- | \n",
    "| **x = 1** | 0.5000 | 0.2500 | 0.1250 | 0.0625 | 0.0312 | 0.0156 | 0.0078 | 0.0039 | 0.0020 | 0.0010 |\n",
    "| **2** | 0.7500 | 0.5000 | 0.3125 | 0.1875 | 0.1094 | 0.0625 | 0.0352 | 0.0195 | 0.0107 | 0.0059 |\n",
    "| **3** | 0.8750 | 0.6875 | 0.5000 | 0.3438 | 0.2266 | 0.1445 | 0.0898 | 0.0547 | 0.0327 | 0.0193 |\n",
    "| **4** | 0.9375 | 0.8125 | 0.6562 | 0.5000 | 0.3633 | 0.2539 | 0.1719 | 0.1133 | 0.0730 | 0.0461 |\n",
    "| **5** | 0.9688 | 0.8906 | 0.7734 | 0.6367 | 0.5000 | 0.3770 | 0.2744 | 0.1938 | 0.1334 | 0.0898 |\n",
    "| **6** | 0.9844 | 0.9375 | 0.8555 | 0.7461 | 0.6230 | 0.5000 | 0.3872 | 0.2905 | 0.2120 | 0.1509 |\n",
    "| **7** | 0.9922 | 0.9648 | 0.9102 | 0.8281 | 0.7256 | 0.6128 | 0.5000 | 0.3953 | 0.3036 | 0.2272 |\n",
    "| **8** | 0.9961 | 0.9805 | 0.9453 | 0.8867 | 0.8062 | 0.7095 | 0.6047 | 0.5000 | 0.4018 | 0.3145 |\n",
    "| **9** | 0.9980 | 0.9893 | 0.9673 | 0.9270 | 0.8666 | 0.7880 | 0.6964 | 0.5982 | 0.5000 | 0.4073 |\n",
    "| **10** | 0.9990 | 0.9941 | 0.9807 | 0.9539 | 0.9102 | 0.8491 | 0.7728 | 0.6855 | 0.5927 | 0.500 0|\n",
    "\n",
    "Just to sanity check these results, I also use Monte Carlo methods to simulate 50,000 games for each set of $(x,y) \\in [1,10] x [1,10]$. The table below presents player $X$'s win probability for each pair of starting token values, and the values are almost identical to the table above.\n",
    "\n",
    "\n",
    "|  | y = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |\n",
    "| -- | ----- | -- | -- | -- | -- | -- | -- | -- | -- | -- | \n",
    "| **x = 1** | 0.5012 | 0.2515 | 0.1240 | 0.0624 | 0.0316 | 0.0156 | 0.0068 | 0.0045 | 0.0022 | 0.0012 | \n",
    "| **2** | 0.7521 | 0.4984 | 0.3085 | 0.1890 | 0.1070 | 0.0638 | 0.0344 | 0.0193 | 0.0106 | 0.0051 | \n",
    "| **3** | 0.8766 | 0.6856 | 0.5003 | 0.3440 | 0.2260 | 0.1443 | 0.0894 | 0.0548 | 0.0332 | 0.0198 | \n",
    "| **4** | 0.9393 | 0.8134 | 0.6561 | 0.4972 | 0.3604 | 0.2510 | 0.1715 | 0.1156 | 0.0709 | 0.0451 | \n",
    "| **5** | 0.9692 | 0.8881 | 0.774 | 0.6369 | 0.4983 | 0.3767 | 0.2728 | 0.1939 | 0.1345 | 0.0901 | \n",
    "| **6** | 0.9842 | 0.9382 | 0.859 | 0.7473 | 0.6237 | 0.4978 | 0.3888 | 0.2887 | 0.2132 | 0.1518 | \n",
    "| **7** | 0.9924 | 0.9654 | 0.9119 | 0.8285 | 0.7272 | 0.6112 | 0.5012 | 0.3982 | 0.3080 | 0.2286 | \n",
    "| **8** | 0.9960 | 0.9816 | 0.9450 | 0.8888 | 0.8077 | 0.7083 | 0.6068 | 0.5030 | 0.4050 | 0.3167 | \n",
    "| **9** | 0.9981 | 0.9893 | 0.9672 | 0.9290 | 0.8673 | 0.7886 | 0.6958 | 0.5970 | 0.5050 | 0.4051 | \n",
    "| **10** | 0.9992 | 0.9941 | 0.9810 | 0.9523 | 0.9107 | 0.8494 | 0.7691 | 0.6867 | 0.5918 | 0.5019 |\n",
    "\n",
    "We can see that when both players start with the same number of tokens, they have 50-50 odds of winning. Additionally, the results are as expected for a game like this, where $X$'s win probability increases down the rows and decreases across the columns. Finally, it is interesting to note for the non-diagonal entries in the matrix $X$ above, $X_{i,j} + X_{j,i} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_game(x, y, n_episodes=50000):\n",
    "    if x <= 0 | y <=0 | (type(x) != int) | (type(y) != int):\n",
    "        raiseValueError('x and y must be positive integers')\n",
    "    wins = np.zeros(n_episodes)\n",
    "    for ep in range(n_episodes): \n",
    "        tokens_x, tokens_y = x, y #Reset token counts\n",
    "        while((tokens_x>0) & (tokens_y>0)):\n",
    "            win_X = np.random.binomial(1, 0.5) #1 if X wins, 0 if Y wins\n",
    "            if win_X:\n",
    "                tokens_y -=1\n",
    "            else:\n",
    "                tokens_x -=1\n",
    "            if tokens_y == 0:\n",
    "                wins[ep] = 1 #Player X wins\n",
    "            elif tokens_x == 0:\n",
    "                wins[ep] = 0 #Player Y wins\n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=1\t0.5012\t0.2515\t0.124\t0.0624\t0.0316\t0.0156\t0.0068\t0.0045\t0.0022\t0.0012\t\n",
      "x=2\t0.7521\t0.4984\t0.3085\t0.189\t0.107\t0.0638\t0.0344\t0.0193\t0.0106\t0.0051\t\n",
      "x=3\t0.8766\t0.6856\t0.5003\t0.344\t0.226\t0.1443\t0.0894\t0.0548\t0.0332\t0.0198\t\n",
      "x=4\t0.9393\t0.8134\t0.6561\t0.4972\t0.3604\t0.251\t0.1715\t0.1156\t0.0709\t0.0451\t\n",
      "x=5\t0.9692\t0.8881\t0.774\t0.6369\t0.4983\t0.3767\t0.2728\t0.1939\t0.1345\t0.0901\t\n",
      "x=6\t0.9842\t0.9382\t0.859\t0.7473\t0.6237\t0.4978\t0.3888\t0.2887\t0.2132\t0.1518\t\n",
      "x=7\t0.9924\t0.9654\t0.9119\t0.8285\t0.7272\t0.6112\t0.5012\t0.3982\t0.308\t0.2286\t\n",
      "x=8\t0.996\t0.9816\t0.945\t0.8888\t0.8077\t0.7083\t0.6068\t0.503\t0.405\t0.3167\t\n",
      "x=9\t0.9981\t0.9893\t0.9672\t0.929\t0.8673\t0.7886\t0.6958\t0.597\t0.505\t0.4051\t\n",
      "x=10\t0.9992\t0.9941\t0.981\t0.9523\t0.9107\t0.8494\t0.7691\t0.6867\t0.5918\t0.5019\t\n"
     ]
    }
   ],
   "source": [
    "x = range(1,11)\n",
    "y = range(1,11)\n",
    "px = np.zeros_like(np.outer(x,y)).astype(float)\n",
    "\n",
    "np.random.seed(1)\n",
    "for i in x:\n",
    "    print('x={}'.format(i), end='\\t')\n",
    "    for j in y:\n",
    "        print(np.round(np.mean(dice_game(i, j)), 4), end='\\t')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
